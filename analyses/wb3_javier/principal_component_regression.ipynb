{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np \n",
    "import nibabel as nib \n",
    "from glob import glob \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import argparse\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "from numpy.testing import assert_array_almost_equal\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.cluster.hierarchy import cut_tree, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist \n",
    "from scipy.stats import spearmanr\n",
    "import nibabel as nib\n",
    "from scipy.cluster.hierarchy import inconsistent\n",
    "\n",
    "\n",
    "def get_pcs_v2(X_train, nComp=None): \n",
    "    # https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com\n",
    "    \n",
    "    #print(\"shape: {}\".format(X_train.shape))\n",
    "    \n",
    "    # we need (TRs x Voxels)\n",
    "    assert(X_train.shape[0] == 180)\n",
    "    \n",
    "    \n",
    "    # create pca object\n",
    "    # nComp determines how many PCs we want ex. 10 \n",
    "    pca = sklearn.decomposition.PCA(n_components=nComp)\n",
    "\n",
    "    # X_train_pca is the nComp PCs ex. 10 \n",
    "    #  X_train_pca.shape == (180, 5)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    assert(X_train_pca.shape == (180,nComp) )\n",
    "    \n",
    "    \n",
    "    # X_projected is the PC projected back into signal space \n",
    "    X_projected = pca.inverse_transform(X_train_pca)\n",
    "    assert(X_projected.shape == X_train.shape)\n",
    "    \n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    assert(exp_var.shape == (nComp,))\n",
    "    \n",
    "    \n",
    "    loss = np.sum((X_train - X_projected) ** 2, axis=1).mean()\n",
    "\n",
    "    # returns\n",
    "    #   PCs\n",
    "    #   the PCs projected to singnal space (same dimensions as input data)\n",
    "    #   explained variance for each PC \n",
    "    #   loss    \n",
    "    return X_train_pca, X_projected, exp_var, loss\n",
    "\n",
    "\n",
    "work_dir=\"/data/NIMH_scratch/kleinrl/ds003216-download/derivatives/sub-02/VASO_fun2_afni\"\n",
    "data_dir=work_dir+\"/data\"\n",
    "\n",
    "out_dir=work_dir+\"/out\"\n",
    "roi_dir=work_dir+\"/rois\"\n",
    "timeseries_maindir=work_dir+\"/timeseries\"\n",
    "plot_dir=work_dir+\"/plots_despike\"\n",
    "\n",
    "img_path = \"/data/NIMH_scratch/kleinrl/gdown/parc_hcp_kenshu_uthr.nii.gz\"\n",
    "img = nib.load(img_path)\n",
    "img_data = img.get_fdata()\n",
    "            \n",
    "\n",
    "sess = glob(timeseries_maindir+\"/VASO_grandmean_WITHOUT-ses-13_spc_despike\")\n",
    "sess+= glob(timeseries_maindir+\"/sub*_spc_despike\")\n",
    "#sess = sess[0:3]\n",
    "\n",
    "roi=\"FEF\"\n",
    "#types = [\"ward\"] #, \"centroid\", \"median\", \"weighted\", \"average\", \"complete\", \"single\" ]\n",
    "types = [\"ward\", \"centroid\", \"median\", \"weighted\", \"average\", \"complete\", \"single\" ]\n",
    "\n",
    "\n",
    "cophenet_coefs = []\n",
    "cophenet_coefs.append(['ses', 'type', 'i', 'corr_coef_sp', 'corr_coef_pe'])\n",
    "\n",
    "ses = sess[0]\n",
    "i   = 30\n",
    "type= \"PCR\"\n",
    "\n",
    "plot_file           = plot_dir+'/clust_'+type+\"_\"+str(i)+\"_\"+ses.split('/')[-1]+\"_\"+roi+\".jpeg\"    \n",
    "plot_file_hist      = plot_dir+'/hist_'+type+\"_\"+str(i)+\"_\"+ses.split('/')[-1]+\"_\"+roi+\".jpeg\"    \n",
    "plot_file_voxs      = plot_dir+'/voxs_'+type+\"_\"+str(i)+\"_\"+ses.split('/')[-1]+\"_\"+roi+\".jpeg\"    \n",
    "plot_nifti_clusters = data_dir+'/clustVoxs2_'+type+\"_\"+str(i)+\"_\"+ses.split('/')[-1]+\"_\"+roi+\".nii.gz\"  \n",
    "\n",
    "\n",
    "#ts_FEF_2D=glob(ses+\"/*\"+roi+\"*.2D\")\n",
    "ts_FEF_2D=glob(ses+\"/*\"+roi+\"*ijk.2D\")\n",
    "ts_FEF_1D=glob(ses+\"/*\"+roi+\"*.1D\")\n",
    "\n",
    "ts_FEF_2D.sort()\n",
    "ts_FEF_1D.sort()\n",
    "\n",
    "\n",
    "FEF     = [ np.loadtxt(x) for x in ts_FEF_2D ] \n",
    "FEF_ind = np.concatenate([len(x)*[y+1] for x,y in zip(FEF, range(0,len(FEF))) ])\n",
    "FEF_cat = np.concatenate(FEF)\n",
    "\n",
    "FEF_ijk = FEF_cat[:,:3]\n",
    "FEF_cat = FEF_cat[:,3:]\n",
    "\n",
    "X_train_pca, X_projected, exp_var, loss       = get_pcs_v2(FEF_cat.T, nComp=i)\n",
    "\n",
    "X = pd.DataFrame(FEF_cat.T)\n",
    "\n",
    "\n",
    "pca = PCA( n_components=10)\n",
    "X_reduced_scale = pca.fit_transform(scale(X))\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "assert_array_almost_equal(X_reduced_scale, X_reduced)\n",
    "\n",
    "\n",
    "\n",
    "#define cross validation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept\n",
    "score = -1*model_selection.cross_val_score(regr,\n",
    "           np.ones((len(X_reduced),1)), y, cv=cv,\n",
    "           scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using cross-validation, adding one component at a time\n",
    "for i in np.arange(1, 6):\n",
    "    score = -1*model_selection.cross_val_score(regr,\n",
    "               X_reduced[:,:i], y, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot cross-validation results    \n",
    "plt.plot(mse)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('hp')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
